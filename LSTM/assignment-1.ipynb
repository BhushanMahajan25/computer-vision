{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5g6gVzuUd03"
      },
      "source": [
        "# LSTM Structure and Hidden State\n",
        "A hidden state is a function of the pieces of data that an LSTM has seen over time; it contains some weights and, represents both the short term and long term memory components for the data that the LSTM has already seen. So, for an LSTM that is looking at words in a sentence, the hidden state of the LSTM will change based on each new word it sees. And, we can use the hidden state to predict the next word in a sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1meD822fVySB"
      },
      "source": [
        "**LSTMs in pyTorch**\n",
        "\n",
        "In PyTorch an LSTM can be defined as: lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=n_layers).\n",
        "All the inputs should be 3D tensors, with dims as follows:\n",
        "\n",
        "\n",
        "*   input_dim = the number of inputs (eg., a dimension of 20 could represent 20 inputs)\n",
        "*   hidden_dim = the size of the hidden state; this will be the number of outputs that each LSTM cell produces at each time step.\n",
        "*   n_layers = the number of hidden LSTM layers to use; this is typically a value between 1 and 3; a value of 1 means that each LSTM cell has one hidden state. This has a default value of 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHZ4u_-lWwQl"
      },
      "source": [
        "**Hidden State**\n",
        "Once an LSTM has been defined with input and hidden dimensions, we can call it and retrieve the output and hidden state at every time step. \n",
        "\n",
        "out, hidden = lstm(input.view(1, 1, -1), (h0, c0))\n",
        "\n",
        "The inputs to an LSTM are (input, (h0, c0)):\n",
        "\n",
        "* input = a Tensor containing the values in an input sequence; this has values: (seq_len, batch, input_size)\n",
        "* h0 = a Tensor containing the initial hidden state for each element in a batch\n",
        "* c0 = a Tensor containing the initial cell memory for each element in the batch\n",
        "\n",
        "h0 nd c0 will default to 0, if they are not specified. Their dimensions are: (n_layers, batch, hidden_dim)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QMPFBjPUJio",
        "outputId": "400b5a9f-cd5b-4176-f467-a0bbd5417b8f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "torch.manual_seed(2) # so that random variables will be consistent and repeatable for testing"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f4de565c8f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a47_vzTvpbFM",
        "outputId": "6062fbc0-7fa7-4e2f-9a30-14dd527be9e0"
      },
      "source": [
        "# The hidden_dim and size of the output will be the same unless \n",
        "# you define your own LSTM and change the number of outputs by adding a linear layer at the end of the network, \n",
        "# ex. fc = nn.Linear(hidden_dim, output_dim).\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "input_dim = 4\n",
        "hidden_dim = 3\n",
        "\n",
        "lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim)\n",
        "\n",
        "input_list = [torch.randn(1, input_dim) for _ in range(5)]\n",
        "print(\"input_list:: \", input_list)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_list::  [tensor([[ 0.7757,  0.9996, -0.2380, -1.7623]]), tensor([[0.4873, 1.4592, 1.4165, 1.0032]]), tensor([[-0.5644,  0.3819,  1.7595,  1.2146]]), tensor([[ 1.0031,  0.0828, -0.5953, -1.5689]]), tensor([[-1.7744, -1.2860, -0.4395,  1.0293]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wWTPz6ZrEgW",
        "outputId": "49da9ef8-bc34-48ac-ede4-75211f52197f"
      },
      "source": [
        "# intialize the hidden state\n",
        "h0 = torch.randn(1, 1, hidden_dim)  # (1 layer, 1 batch_size, 3 outputs)\n",
        "c0 = torch.randn(1, 1 ,hidden_dim)  # cell memory\n",
        "\n",
        "h0 = Variable(h0)\n",
        "c0 = Variable(c0)\n",
        "\n",
        "for i in input_list:\n",
        "  i = Variable(i)\n",
        "  out, hidden = lstm(i.view(1, 1, -1), (h0, c0))\n",
        "  # the output and hidden Tensors are always of length 3, which we specified when we defined the LSTM with hidden_dim.\n",
        "  print(\"\\nout:: \",out)\n",
        "  print(\"hidden:: \",hidden)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "out::  tensor([[[-0.1500, -0.0961,  0.2719]]], grad_fn=<StackBackward>)\n",
            "hidden::  (tensor([[[-0.1500, -0.0961,  0.2719]]], grad_fn=<StackBackward>), tensor([[[-0.3462, -0.1863,  0.8630]]], grad_fn=<StackBackward>))\n",
            "\n",
            "out::  tensor([[[ 0.0745, -0.2756,  0.4170]]], grad_fn=<StackBackward>)\n",
            "hidden::  (tensor([[[ 0.0745, -0.2756,  0.4170]]], grad_fn=<StackBackward>), tensor([[[ 0.2830, -0.5666,  0.8108]]], grad_fn=<StackBackward>))\n",
            "\n",
            "out::  tensor([[[ 0.0724, -0.3120,  0.2302]]], grad_fn=<StackBackward>)\n",
            "hidden::  (tensor([[[ 0.0724, -0.3120,  0.2302]]], grad_fn=<StackBackward>), tensor([[[ 0.5445, -0.6505,  0.3713]]], grad_fn=<StackBackward>))\n",
            "\n",
            "out::  tensor([[[-0.0685,  0.0069,  0.3191]]], grad_fn=<StackBackward>)\n",
            "hidden::  (tensor([[[-0.0685,  0.0069,  0.3191]]], grad_fn=<StackBackward>), tensor([[[-0.1566,  0.0116,  0.6756]]], grad_fn=<StackBackward>))\n",
            "\n",
            "out::  tensor([[[ 0.1004, -0.1014,  0.2815]]], grad_fn=<StackBackward>)\n",
            "hidden::  (tensor([[[ 0.1004, -0.1014,  0.2815]]], grad_fn=<StackBackward>), tensor([[[ 0.5204, -0.2779,  0.4855]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1e33FsDss9L"
      },
      "source": [
        "A for loop is not very efficient for large sequences of data, so we can also, process all of these inputs at once.\n",
        "\n",
        "1. concatenate all our input sequences into one big tensor, with a defined batch_size\n",
        "2. define the shape of our hidden state\n",
        "3. get the outputs and the most recent hidden state (created after the last word in the sequence has been seen)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6WW6kLNtAGk",
        "outputId": "15a9772d-c093-400a-8339-0a5cd6b89615"
      },
      "source": [
        "# turn inputs into a tensor with 5 rows of data\n",
        "# add the extra 2nd dimension (1) for batch_size\n",
        "\n",
        "# torch.cat(tensors, dim=0, *, out=None) → Tensor :: Concatenates the given sequence of seq tensors in the given dimension. \n",
        "#   All tensors must either have the same shape (except in the concatenating dimension) or be empty.\n",
        "\n",
        "# view(*shape) → Tensor :: Returns a new tensor with the same data as the self tensor but of a different shape.\n",
        "# PyTorch allows a tensor to be a View of an existing tensor. \n",
        "#   View tensor shares the same underlying data with its base tensor. \n",
        "#     Supporting View avoids explicit data copy, \n",
        "#       thus allows us to do fast and memory efficient reshaping, slicing and element-wise operations.\n",
        "# Since views share underlying data with its base tensor, if you edit the data in the view, \n",
        "#   it will be reflected in the base tensor as well.\n",
        "inputs = torch.cat(input_list).view(len(input_list), 1, -1)\n",
        "\n",
        "# print out our inputs and their shape\n",
        "# you should see (number of sequences, batch size, input_dim)\n",
        "print('inputs size: \\n', inputs.size())\n",
        "print('\\n')\n",
        "\n",
        "print('inputs: \\n', inputs)\n",
        "print('\\n')\n",
        "\n",
        "# initialize the hidden state\n",
        "h0 = torch.randn(1, 1, hidden_dim)\n",
        "c0 = torch.randn(1, 1, hidden_dim)\n",
        "\n",
        "# wrap everything in Variable\n",
        "inputs = Variable(inputs)\n",
        "h0 = Variable(h0)\n",
        "c0 = Variable(c0)\n",
        "# get the outputs and hidden state\n",
        "out, hidden = lstm(inputs, (h0, c0))\n",
        "\n",
        "print('out: \\n', out)\n",
        "print('hidden: \\n', hidden)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inputs size: \n",
            " torch.Size([5, 1, 4])\n",
            "\n",
            "\n",
            "inputs: \n",
            " tensor([[[ 0.7757,  0.9996, -0.2380, -1.7623]],\n",
            "\n",
            "        [[ 0.4873,  1.4592,  1.4165,  1.0032]],\n",
            "\n",
            "        [[-0.5644,  0.3819,  1.7595,  1.2146]],\n",
            "\n",
            "        [[ 1.0031,  0.0828, -0.5953, -1.5689]],\n",
            "\n",
            "        [[-1.7744, -1.2860, -0.4395,  1.0293]]])\n",
            "\n",
            "\n",
            "out: \n",
            " tensor([[[-0.2600, -0.0558,  0.0722]],\n",
            "\n",
            "        [[-0.0359, -0.1937,  0.0391]],\n",
            "\n",
            "        [[ 0.0771, -0.2851, -0.3132]],\n",
            "\n",
            "        [[-0.0210, -0.0216, -0.0668]],\n",
            "\n",
            "        [[ 0.0898,  0.0230, -0.1873]]], grad_fn=<StackBackward>)\n",
            "hidden: \n",
            " (tensor([[[ 0.0898,  0.0230, -0.1873]]], grad_fn=<StackBackward>), tensor([[[ 0.2407,  0.0637, -0.2616]]], grad_fn=<StackBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}