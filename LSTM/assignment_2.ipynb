{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python386jvsc74a57bd0d1f7251d3492186c7ce10244c1216acb155820869ed32edfd9f4e92f93eefccb",
      "display_name": "Python 3.8.6 64-bit ('venv2')"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.6-candidate1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f01LHmTCNt-b"
      },
      "source": [
        "# **LSTM for Part-of-Speech Tagging**\n",
        "Part of speech tagging is the process of determining the category of a word from the words in its surrounding context.\n",
        "\n",
        "**Why Tag Speech?**\n",
        "* Often used to help disambiguate natural language phrases because it can be done quickly and with high accuracy. \n",
        "* It can help answer: what subject is someone talking about?\n",
        "* Tagging can be used for many NLP tasks like creating new sentences using a sequence of tags that make sense together, filling in a Mad Libs style game, and determining correct pronunciation during speech synthesis. \n",
        "* It is also used in information retrieval, and for word disambiguation (ex. determining when someone says right like the direction versus right like \"that's right!\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXoOyvQWYr73"
      },
      "source": [
        "# **Preparing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6R_mYeaNXYv"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJUmAPzkPfco",
        "outputId": "c7717888-897a-43b6-9082-b20ea0ee1eb6"
      },
      "source": [
        "# training sentences and their corresponding word-tags\n",
        "# DET: Determinant, NN: Noun, V: Verb\n",
        "training_data = [\n",
        "  (\"The cat ate the cheese\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "  (\"She read that book\".lower().split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
        "  (\"The dog loves art\".lower().split(), [\"DET\", \"NN\", \"V\", \"NN\"]),\n",
        "  (\"The elephant answers the phone\".lower().split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"])\n",
        "]\n",
        "\n",
        "print(\"training_data:: \", training_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training_data::  [(['the', 'cat', 'ate', 'the', 'cheese'], ['DET', 'NN', 'V', 'DET', 'NN']), (['she', 'read', 'that', 'book'], ['NN', 'V', 'DET', 'NN']), (['the', 'dog', 'loves', 'art'], ['DET', 'NN', 'V', 'NN']), (['the', 'elephant', 'answers', 'the', 'phone'], ['DET', 'NN', 'V', 'DET', 'NN'])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0TF7h4oRIpc",
        "outputId": "fe1da2df-597e-4918-cdda-60d8881fcfdb"
      },
      "source": [
        "# create a dictionary that maps words to indices\n",
        "word2idx = {}\n",
        "for sent, tags in training_data:\n",
        "  for word in sent:\n",
        "    if word not in word2idx:\n",
        "      word2idx[word] = len(word2idx)\n",
        "\n",
        "print(\"word2idx:: \", word2idx)\n",
        "\n",
        "# create a dictionary that maps tags to indices\n",
        "tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word2idx::  {'the': 0, 'cat': 1, 'ate': 2, 'cheese': 3, 'she': 4, 'read': 5, 'that': 6, 'book': 7, 'dog': 8, 'loves': 9, 'art': 10, 'elephant': 11, 'answers': 12, 'phone': 13}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F40YEn31R2oh"
      },
      "source": [
        "import numpy as np\n",
        "# a helper function for converting a sequence of words to a Tensor of numerical values\n",
        "# will be used later in training\n",
        "\n",
        "def prepare_sequence(seq, to_idx):\n",
        "  '''This function takes in a sequence of words and returns a \n",
        "    corresponding Tensor of numerical values (indices for each word).'''\n",
        "  idxs = [to_idx[w] for w in seq]\n",
        "  idxs = np.array(idxs)\n",
        "  return torch.from_numpy(idxs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "um7KqUE-Tt6N",
        "outputId": "ee6c29b5-b103-48c3-d38f-57aaae96c81a"
      },
      "source": [
        "example_input = prepare_sequence(\"The dog answers the phone\".lower().split(), word2idx)\n",
        "print(\"example_input\", example_input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "example_input tensor([ 0,  8, 12,  0, 13])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BNsaAHZVLdo"
      },
      "source": [
        "---\n",
        "\n",
        "# **Creating the Model**\n",
        "Our model will assume a few things:\n",
        "1. Our input is broken down into a sequence of words, so a sentence will be [w1, w2, ...]\n",
        "2. These words come from a larger list of words that we already know (a vocabulary)\n",
        "3. We have a limited set of tags, [NN, V, DET], which mean: a noun, a verb, and a determinant (words like \"the\" or \"that\"), respectively\n",
        "4. We want to predict a tag for each input word\n",
        "\n",
        "To do the prediction, we will pass an LSTM over a test sentence and apply a softmax function to the hidden state of the LSTM; the result is a vector of tag scores from which we can get the predicted tag for a word based on the maximum value in this distribution of tag scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_UV0I_xZONb"
      },
      "source": [
        "**Word embeddings**\n",
        "\n",
        "LSTM takes in an expected input size and hidden_dim, but sentences are rarely of a consistent size.\n",
        "\n",
        "At the very start of this net, we'll create an Embedding layer that takes in the size of our vocabulary and returns a vector of a specified size, embedding_dim, for each word in an input sequence of words. \n",
        "\n",
        "It's important that this be the first layer in this net. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K026q19RU9l2"
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "  def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "    super(LSTMTagger,self).__init__()\n",
        "\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    # embedding layer that turns words into a vector of a specified size\n",
        "    self.word_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "\n",
        "    # the LSTM takes embedded word vectors (of a specified size) as inputs \n",
        "      # and outputs hidden states of size hidden_dim\n",
        "    self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim)\n",
        "\n",
        "    # the linear layer that maps the hidden state output dimension \n",
        "      # to the number of tags we want as output, tagset_size (in this case this is 3 tags)\n",
        "    self.hidden2tag = nn.Linear(in_features=hidden_dim, out_features=tagset_size)\n",
        "\n",
        "    # initialize the hidden state\n",
        "    self.hidden = self.init_hidden()\n",
        "\n",
        "  def init_hidden(self):\n",
        "    ''' At the start of training, we need to initialize a hidden state;\n",
        "        there will be none because the hidden state is formed based on perviously seen data.\n",
        "        So, this function defines a hidden state with all zeroes and of a specified size.'''\n",
        "    # The axes dimensions are (n_layers, batch_size, hidden_dim)\n",
        "    return (torch.zeros(1, 1, self.hidden_dim), torch.zeros(1, 1, self.hidden_dim))\n",
        "\n",
        "  def forward(self, sentence):\n",
        "    # create embedded word vectors for each word in a sentence\n",
        "    embeds = self.word_embeddings(sentence)\n",
        "\n",
        "    # get the output and hidden state by passing the lstm over our word embeddings\n",
        "      # the lstm takes in our embeddings and hiddent state\n",
        "    lstm_out, self.hidden = self.lstm(embeds.view(len(sentence), 1, -1), self.hidden)\n",
        "\n",
        "    # get the scores for the most likely tag for a word\n",
        "    tag_outputs = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "    tag_scores = F.log_softmax(input=tag_outputs, dim=1)\n",
        "\n",
        "    return tag_scores\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CliEdhwKgaJq"
      },
      "source": [
        "# the embedding dimension defines the size of our word vectors\n",
        "# for our simple vocabulary and training set, we will keep these small\n",
        "\n",
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "\n",
        "# instantiate our model\n",
        "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), len(tag2idx))\n",
        "\n",
        "# define loss and optimizer\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJGSwhLIhnSW",
        "outputId": "6bff346f-5c3d-47fe-f868-8c3a29540789"
      },
      "source": [
        "test_sentence = \"The cheese loves the elephant\".lower().split()\n",
        "\n",
        "# see what the scores are before training\n",
        "# element [i,j] of the output is the *score* for tag j for word i.\n",
        "# to check the initial accuracy of our model, we don't need to train, so we use model.eval()\n",
        "inputs = prepare_sequence(test_sentence, word2idx)\n",
        "inputs = inputs\n",
        "tag_scores = model(inputs)\n",
        "print(\"tag_scores\", tag_scores)\n",
        "\n",
        "# tag_scores outputs a vector of tag scores for each word in an inpit sentence\n",
        "# to get the most likely tag index, we grab the index with the maximum score!\n",
        "# recall that these numbers correspond to tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
        "_, predicted_tags = torch.max(tag_scores, 1)\n",
        "print('\\n')\n",
        "print('Predicted tags: \\n',predicted_tags)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tag_scores tensor([[-1.4876, -1.1221, -0.8019],\n",
            "        [-1.6764, -1.1624, -0.6927],\n",
            "        [-1.5466, -1.1654, -0.7440],\n",
            "        [-1.5359, -1.1410, -0.7652],\n",
            "        [-1.5345, -1.0699, -0.8178]], grad_fn=<LogSoftmaxBackward>)\n",
            "\n",
            "\n",
            "Predicted tags: \n",
            " tensor([2, 2, 2, 2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJOL1KxAkBkw",
        "outputId": "2398abf5-b435-4fe9-bb44-e34542e3de3c"
      },
      "source": [
        "# normally these epochs take a lot longer \n",
        "# but with our toy data (only 3 sentences), we can do many epochs in a short time\n",
        "n_epochs = 300\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    \n",
        "    # get all sentences and corresponding tags in the training data\n",
        "    for sentence, tags in training_data:\n",
        "        \n",
        "        # zero the gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # zero the hidden state of the LSTM, this detaches it from its history\n",
        "        model.hidden = model.init_hidden()\n",
        "\n",
        "        # prepare the inputs for processing by out network, \n",
        "        # turn all sentences and targets into Tensors of numerical indices\n",
        "        sentence_in = prepare_sequence(sentence, word2idx)\n",
        "        targets = prepare_sequence(tags, tag2idx)\n",
        "\n",
        "        # forward pass to get tag scores\n",
        "        tag_scores = model(sentence_in)\n",
        "\n",
        "        # compute the loss, and gradients \n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        epoch_loss += loss.item()\n",
        "        loss.backward()\n",
        "        \n",
        "        # update the model parameters with optimizer.step()\n",
        "        optimizer.step()\n",
        "        \n",
        "    # print out avg loss per 20 epochs\n",
        "    if(epoch%20 == 19):\n",
        "        print(\"Epoch: %d, loss: %1.5f\" % (epoch+1, epoch_loss/len(training_data)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 20, loss: 1.10391\n",
            "Epoch: 40, loss: 1.06142\n",
            "Epoch: 60, loss: 1.03421\n",
            "Epoch: 80, loss: 1.01339\n",
            "Epoch: 100, loss: 0.99488\n",
            "Epoch: 120, loss: 0.97664\n",
            "Epoch: 140, loss: 0.95764\n",
            "Epoch: 160, loss: 0.93733\n",
            "Epoch: 180, loss: 0.91548\n",
            "Epoch: 200, loss: 0.89204\n",
            "Epoch: 220, loss: 0.86711\n",
            "Epoch: 240, loss: 0.84087\n",
            "Epoch: 260, loss: 0.81356\n",
            "Epoch: 280, loss: 0.78547\n",
            "Epoch: 300, loss: 0.75686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGieSUrMkIGY",
        "outputId": "73f38f32-9b52-4771-ab5f-031d8fbf0f5a"
      },
      "source": [
        "test_sentence = \"The cheese loves the elephant\".lower().split()\n",
        "\n",
        "# see what the scores are after training\n",
        "inputs = prepare_sequence(test_sentence, word2idx)\n",
        "inputs = inputs\n",
        "tag_scores = model(inputs)\n",
        "print(tag_scores)\n",
        "\n",
        "# print the most likely tag index, by grabbing the index with the maximum score!\n",
        "# recall that these numbers correspond to tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
        "_, predicted_tags = torch.max(tag_scores, 1)\n",
        "print('\\n')\n",
        "print('Predicted tags: \\n',predicted_tags)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.5204, -1.2120, -2.2246],\n",
            "        [-1.5089, -0.4769, -1.8444],\n",
            "        [-1.5630, -0.6654, -1.2859],\n",
            "        [-0.6430, -1.1018, -1.9518],\n",
            "        [-1.6399, -0.5664, -1.4336]], grad_fn=<LogSoftmaxBackward>)\n",
            "\n",
            "\n",
            "Predicted tags: \n",
            " tensor([0, 1, 1, 0, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}